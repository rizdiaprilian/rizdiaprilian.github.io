[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is the gathering of my journey I have taken in machine learning. This is built with a sole purpose of solidify my affinity to this field, and writing is a great way to do."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rizdi Aprilian ML Journey",
    "section": "",
    "text": "Start to write\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nBuilding XGBoost Package with BentoML\n\n\n\n\n\n\n\nXGBoost\n\n\ncode\n\n\nBentoML\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nRizdi Aprilian\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Start to write",
    "section": "",
    "text": "Begin to write\n\nprint(\"Hello World\")\n\nHello World"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/XGBoost_to_BentoML/index.html",
    "href": "posts/XGBoost_to_BentoML/index.html",
    "title": "Building XGBoost Package with BentoML",
    "section": "",
    "text": "After completing the training of the algorithm on the dataset, the next crucial step is deploying the model to serve predictions within an API interface. However, deploying the model using Flask application poses scalability challenges. When faced with high traffic calls, especially in production environments where hundreds or even thousands of requests may arrive simultaneously, Flask’s WSGI standard hampers its ability to receive new requests until the previous ones are completed. This limitation restricts the service’s capacity to handle parallel requests efficiently.\nTo overcome this challenge, a successor to WSGI called ASGI was introduced. ASGI allows for the handling of multiple requests simultaneously by leveraging its asynchronous behavior. This modern web framework provides improved efficiency while still retaining the essential features of WSGI when required. In response to these advancements, BentoML emerged as a solution.\nBentoML propose functionality that allows data scientists and ML engineers to:\n\nMake transition from experiment in notebook to ML service with scalability in standard way. You can create and package your ML service useful for production.\nCustomize ML service to fit well with specific use cases.\nVerify that ML service is ready for production. This includes deployment, monitoring, and operation.\n\nThis notebook will demonstrate how making XGBoost service package is possible with BentoML.\n\nImporting Libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\n\nLoading Data Credit Scoring\n\ndata = 'CreditScoring.csv'\n\ndf = pd.read_csv(data)\ndf.columns = df.columns.str.lower()\n\n\n\nData Preprocessing\nLet’s create a mapping to modify categorical variables.\n\nstatus_values = {\n    1: 'ok',\n    2: 'default',\n    0: 'unk'\n}\n\ndf.status = df.status.map(status_values)\n\n\nhome_values = {\n    1: 'rent',\n    2: 'owner',\n    3: 'private',\n    4: 'ignore',\n    5: 'parents',\n    6: 'other',\n    0: 'unk'\n}\n\ndf.home = df.home.map(home_values)\n\nmarital_values = {\n    1: 'single',\n    2: 'married',\n    3: 'widow',\n    4: 'separated',\n    5: 'divorced',\n    0: 'unk'\n}\n\ndf.marital = df.marital.map(marital_values)\n\nrecords_values = {\n    1: 'no',\n    2: 'yes',\n    0: 'unk'\n}\n\ndf.records = df.records.map(records_values)\n\njob_values = {\n    1: 'fixed',\n    2: 'partime',\n    3: 'freelance',\n    4: 'others',\n    0: 'unk'\n}\n\ndf.job = df.job.map(job_values)\n\nA simple descriptive distribution on numerical features.\n\ndf.describe().round()\n\n\n\n\n\n\n\n\nseniority\ntime\nage\nexpenses\nincome\nassets\ndebt\namount\nprice\n\n\n\n\ncount\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n\n\nmean\n8.0\n46.0\n37.0\n56.0\n763317.0\n1060341.0\n404382.0\n1039.0\n1463.0\n\n\nstd\n8.0\n15.0\n11.0\n20.0\n8703625.0\n10217569.0\n6344253.0\n475.0\n628.0\n\n\nmin\n0.0\n6.0\n18.0\n35.0\n0.0\n0.0\n0.0\n100.0\n105.0\n\n\n25%\n2.0\n36.0\n28.0\n35.0\n80.0\n0.0\n0.0\n700.0\n1118.0\n\n\n50%\n5.0\n48.0\n36.0\n51.0\n120.0\n3500.0\n0.0\n1000.0\n1400.0\n\n\n75%\n12.0\n60.0\n45.0\n72.0\n166.0\n6000.0\n0.0\n1300.0\n1692.0\n\n\nmax\n48.0\n72.0\n68.0\n180.0\n99999999.0\n99999999.0\n99999999.0\n5000.0\n11140.0\n\n\n\n\n\n\n\nThere are a few extreme values on column income, assets, and debt. We replace them to NaN values.\n\nfor c in ['income', 'assets', 'debt']:\n    df[c] = df[c].replace(to_replace=99999999, value=np.nan)\n\n\ndf.isnull().sum()\n\nstatus        0\nseniority     0\nhome          0\ntime          0\nage           0\nmarital       0\nrecords       0\njob           0\nexpenses      0\nincome       34\nassets       47\ndebt         18\namount        0\nprice         0\ndtype: int64\n\n\n\ndf = df[df.status != 'unk'].reset_index(drop=True)\n\nA more meaningful description.\n\ndf.describe().round()\n\n\n\n\n\n\n\n\nseniority\ntime\nage\nexpenses\nincome\nassets\ndebt\namount\nprice\n\n\n\n\ncount\n4454.0\n4454.0\n4454.0\n4454.0\n4420.0\n4407.0\n4436.0\n4454.0\n4454.0\n\n\nmean\n8.0\n46.0\n37.0\n56.0\n131.0\n5404.0\n343.0\n1039.0\n1463.0\n\n\nstd\n8.0\n15.0\n11.0\n20.0\n86.0\n11574.0\n1246.0\n475.0\n628.0\n\n\nmin\n0.0\n6.0\n18.0\n35.0\n0.0\n0.0\n0.0\n100.0\n105.0\n\n\n25%\n2.0\n36.0\n28.0\n35.0\n80.0\n0.0\n0.0\n700.0\n1117.0\n\n\n50%\n5.0\n48.0\n36.0\n51.0\n120.0\n3000.0\n0.0\n1000.0\n1400.0\n\n\n75%\n12.0\n60.0\n45.0\n72.0\n165.0\n6000.0\n0.0\n1300.0\n1692.0\n\n\nmax\n48.0\n72.0\n68.0\n180.0\n959.0\n300000.0\n30000.0\n5000.0\n11140.0\n\n\n\n\n\n\n\n\n\nData Splitting\nWe divide the date to three sets: train, validation and test.\n\nfrom sklearn.model_selection import train_test_split\n\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=11)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=11)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = (df_train.status == 'default').astype('int').values\ny_val = (df_val.status == 'default').astype('int').values\ny_test = (df_test.status == 'default').astype('int').values\n\ndel df_train['status']\ndel df_val['status']\ndel df_test['status']\n\nTurn pandas dataframe to dictionary format.\n\nfrom sklearn.feature_extraction import DictVectorizer\n\n# Filling missing values with 0\ndict_train = df_train.fillna(0).to_dict(orient='records')\ndict_val = df_val.fillna(0).to_dict(orient='records')\n\nSlice the first dictionary of feature values from training set to see what it looks like.\n\ndict_train[0]\n\n{'seniority': 10,\n 'home': 'owner',\n 'time': 36,\n 'age': 36,\n 'marital': 'married',\n 'records': 'no',\n 'job': 'freelance',\n 'expenses': 75,\n 'income': 0.0,\n 'assets': 10000.0,\n 'debt': 0.0,\n 'amount': 1000,\n 'price': 1400}\n\n\nDictVectorizer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays.\n\ndv = DictVectorizer(sparse=False)\n\nX_train = dv.fit_transform(dict_train)\nX_val = dv.transform(dict_val)\n\n\nX_train[0]\n\narray([3.6e+01, 1.0e+03, 1.0e+04, 0.0e+00, 7.5e+01, 0.0e+00, 0.0e+00,\n       1.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n       1.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 1.0e+00, 0.0e+00,\n       0.0e+00, 0.0e+00, 0.0e+00, 1.4e+03, 1.0e+00, 0.0e+00, 1.0e+01,\n       3.6e+01])\n\n\n\n\nGradient Boosting and XGBoost\n\nGradient boosting vs random forest\nInstalling XGBoost\nTraining the first model\nPerformance monitoring\nParsing xgboost’s monitoring output\n\nXGBoost requires a data structure suitable for both memory efficieny and training speed. Thankfully, XGBoost provides a way to convert different data sources to the structure DMatrix.\nNote:\nFeature names is no longer required to construct train and validation matrices\nIt was\nfeatures = dv.get_feature_names_out()\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=features)\nNow it’s\ndtrain = xgb.DMatrix(X_train, label=y_train)\n\nimport xgboost as xgb\n\nfeatures = dv.get_feature_names_out()\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\n\nBegin fitting XGBoost to the data.\n\n# Provide parameters for XGBoost\nxgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n    \n    'objective': 'binary:logistic',\n    'nthread': 8,\n    \n    'seed': 1,\n    'verbosity': 1,\n}\n\n# Train XGBoost to train matrices\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=50)\n\nPredict XGBoost model that has been trained on the validation set.\n\ny_pred = model.predict(dval)\ny_pred[:10]\n\narray([0.04114904, 0.01547354, 0.02859539, 0.14339729, 0.0053675 ,\n       0.03264806, 0.00689159, 0.53664714, 0.52047443, 0.00165206],\n      dtype=float32)\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_val, y_pred)\n\n0.8114570971882493\n\n\n\nwatchlist = [(dtrain, 'train'), (dval, 'val')]\n\n\n%%capture output\n\nxgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n    \n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n\n    'nthread': 8,\n    'seed': 1,\n    'verbosity': 1,\n}\n\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                  verbose_eval=10,\n                  evals=watchlist)\n\n\ns = output.stdout\n\nprint(s[:100])\n\n[0] train-auc:0.86300   val-auc:0.76818\n[10]    train-auc:0.95002   val-auc:0.81558\n[20]    train-auc:0.97316   \n\n\n\ndef parse_xgb_output(output):\n    results = []\n\n    for line in output.stdout.strip().split('\\n'):\n        it_line, train_line, val_line = line.split('\\t')\n\n        it = int(it_line.strip('[]'))\n        train = float(train_line.split(':')[1])\n        val = float(val_line.split(':')[1])\n\n        results.append((it, train, val))\n    \n    columns = ['num_iter', 'train_auc', 'val_auc']\n    df_results = pd.DataFrame(results, columns=columns)\n    return df_results\n\n\ndf_score = parse_xgb_output(output)\n\n\nplt.plot(df_score.num_iter, df_score.train_auc, color=\"green\", linestyle=\"solid\", label='Train AUC')\nplt.plot(df_score.num_iter, df_score.val_auc, color=\"orange\", linestyle=\"dashed\", label='Validation AUC')\nplt.legend()\n\nplt.title('XGBoost: number of trees vs AUC')\nplt.xlabel('Number of trees')\nplt.ylabel('AUC')\n\n# plt.savefig('ch06-figures/06_xgb_default.svg')\n\nplt.show()\n\n\n\n\n\nplt.plot(df_score.num_iter, df_score.val_auc, label='Validation AUC')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1dc4a0b0550&gt;\n\n\n\n\n\n\n\nSaving a XGBoost model with BentoML\nAfter training, use save_model() to save the Booster instance to BentoML model store. XGBoost has no framework-specific save options.\n\nimport bentoml\n\nbentoml.xgboost.save_model(\"credit_scoring_model\", model,\n                        custom_objects={\n                            \"dictVectorizer\": dv\n                            })\n\nModel(tag=\"credit_scoring_model:xbdou5qemcv5o7fs\", path=\"C:\\Users\\rizdi\\bentoml\\models\\credit_scoring_model\\xbdou5qemcv5o7fs\\\")\n\n\n\n### If prediction on batches is preferred\n\nbentoml.xgboost.save_model(\"credit_scoring_model\", model,\n                            custom_objects={\n                                \"dictVectorizer\": dv\n                            },\n                            signatures={\n                                \"predict\": {\n                                    \"batchable\": True,\n                                    \"batch_dim\": 0,\n                                }\n                            }\n                        )"
  }
]
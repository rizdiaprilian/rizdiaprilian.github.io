[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is the gathering of my journey I have taken in machine learning. This is built with a sole purpose of solidify my affinity to this field, and writing is a great way to do."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rizdi Aprilian ML Journey",
    "section": "",
    "text": "Start to write\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHyperparameter Optimization with Optuna\n\n\n\n\n\n\n\nHyperparameter\n\n\nOptuna\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2023\n\n\nRizdi Aprilian\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nBuilding XGBoost Package with BentoML\n\n\n\n\n\n\n\nXGBoost\n\n\ncode\n\n\nBentoML\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nRizdi Aprilian\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Start to write",
    "section": "",
    "text": "Begin to write\n\nprint(\"Hello World\")\n\nHello World"
  },
  {
    "objectID": "posts/Optimization_with_Optuna/index.html",
    "href": "posts/Optimization_with_Optuna/index.html",
    "title": "Hyperparameter Optimization with Optuna",
    "section": "",
    "text": "In developing predictive machine learning, a practice that many could not overlook at is pursuing a hyperparameter that return the best possible of optimum prediction on data. Grid search and Randomized search have been greatly relied on for quite some time. Grid search selects a set collection of values to test for each parameter by creating permutations of each value from each searching group, as for many perceive this method as brute-force-search. Random search, meanwhile, performs testing the effects of different hyperparameters at the same time using random sampling of each group, and this technique comes at the cost of suboptimal training while greatly reduce computational load and time required to run grid search.\nThat’s where Optuna fill the gap. Known from many as framework agnostic (meaning that regardless of machine learning libraries you’re using), Optuna is specifically designed for running hyperparameter optimization in automatic fashion. What’s more is that Optuna also supports for parallelism - scaling this task from one machine to multiple machine in a cluster. However, in this page, I’ll demonstrated how Optuna can assist in arriving at hyperparameter with optimum prediction in local machine. LGBM is chosen.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n\nimport optuna\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100)\nplt.style.use('fivethirtyeight')\n\nc:\\Users\\rizdi\\miniconda3\\envs\\mlops_monitor\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\ndf = pd.read_csv('normalized_data.csv', sep=',')\ndf.head(5)\n\n\n\n\n\n\n\n\nage\nanaemia\ncreatinine_phosphokinase\ndiabetes\nejection_fraction\nhigh_blood_pressure\nplatelets\nserum_creatinine\nserum_sodium\nsex\nsmoking\ntime\nDEATH_EVENT\n\n\n\n\n0\n75.0\n0\n582\n0\n20\n1\n265000.00\n1.9\n130\n1\n0\n4\n1\n\n\n1\n55.0\n0\n7861\n0\n38\n0\n263358.03\n1.1\n136\n1\n0\n6\n1\n\n\n2\n65.0\n0\n146\n0\n20\n0\n162000.00\n1.3\n129\n1\n1\n7\n1\n\n\n3\n50.0\n1\n111\n0\n20\n0\n210000.00\n1.9\n137\n1\n0\n7\n1\n\n\n4\n65.0\n1\n160\n1\n20\n0\n327000.00\n2.7\n116\n0\n0\n8\n1\ncat_cols = list(filter(lambda x: x if len(df[x].unique()) &lt;= 3 else None, df.columns))\n\ndf[cat_cols] = df[cat_cols].astype('category')\nX = df.loc[:,:\"time\"]\ny = df.loc[:,[\"DEATH_EVENT\"]]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((239, 12), (60, 12), (239, 1), (60, 1))\ncat_columns = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\nnum_columns = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n## Transformer Pipeline\nnum_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ncat_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(drop='first', dtype=np.int))\n])\n\n## Column Transformer\npreprocessor = ColumnTransformer([\n    ('numeric', num_transformer, num_columns),\n    ('categoric', cat_transformer, cat_columns),\n])\n\n## Apply Column Transformer\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n## Label Encoding\ny_transformer = LabelEncoder()\ny_train = y_transformer.fit_transform(y_train).ravel()\ny_test = y_transformer.transform(y_test).ravel()\ndef objective(trial):\n    params = {\n        'random_state': 23,\n        'n_estimators': 200,\n        'reg_alpha': trial.suggest_float('reg_alpha', 1E-10, 1E-5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1E-10, 1E-5),\n        'num_leaves': trial.suggest_int('num_leaves', 150, 300),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 10,50),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 0.1),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),\n        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n        'objective': 'binary',\n        'metric': 'binary_logloss'\n    }\n    \n    # Create the  LGBMClassifier learning object\n    lgbm_model = LGBMClassifier(**params)\n    \n    lgbm_model.fit(X_train, y_train)\n    \n    # Make predictions on the validation set\n    y_pred = lgbm_model.predict(X_test)\n\n    # Calculate MAE as the evaluation metric\n    roc_auc = roc_auc_score(y_test, y_pred)\n\n    # Return the evaluation metric value as the objective value to be minimized\n    return roc_auc\n%%time\n\n# Create an Optuna study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\n[I 2023-06-17 13:34:12,545] A new study created in memory with name: no-name-f9f4f083-528e-4b8a-bce8-2ade3b89f346\n[I 2023-06-17 13:34:12,709] Trial 0 finished with value: 0.7246469833119383 and parameters: {'reg_alpha': 5.270142430321572e-06, 'reg_lambda': 7.0947209053917484e-06, 'num_leaves': 178, 'learning_rate': 0.0891781733041886, 'max_depth': 35, 'colsample_bytree': 0.03090950619557188, 'subsample': 0.24825549471540287, 'min_child_samples': 20, 'subsample_freq': 9}. Best is trial 0 with value: 0.7246469833119383.\n[I 2023-06-17 13:34:13,227] Trial 1 finished with value: 0.6861360718870345 and parameters: {'reg_alpha': 7.025236249526668e-06, 'reg_lambda': 4.571522404954949e-06, 'num_leaves': 256, 'learning_rate': 0.04958566829222305, 'max_depth': 25, 'colsample_bytree': 0.07986235314153306, 'subsample': 0.4107035959692341, 'min_child_samples': 3, 'subsample_freq': 8}. Best is trial 0 with value: 0.7246469833119383.\n[I 2023-06-17 13:34:13,341] Trial 2 finished with value: 0.5 and parameters: {'reg_alpha': 6.554675169962187e-07, 'reg_lambda': 6.906462441481608e-06, 'num_leaves': 176, 'learning_rate': 0.033577259275936314, 'max_depth': 29, 'colsample_bytree': 0.024786159372631178, 'subsample': 0.12259677265568039, 'min_child_samples': 20, 'subsample_freq': 8}. Best is trial 0 with value: 0.7246469833119383.\n[I 2023-06-17 13:34:13,513] Trial 3 finished with value: 0.5808729139922978 and parameters: {'reg_alpha': 3.2574904058589524e-06, 'reg_lambda': 8.436676162704015e-06, 'num_leaves': 205, 'learning_rate': 0.02214270724529358, 'max_depth': 41, 'colsample_bytree': 0.09762750003330573, 'subsample': 0.953600458589063, 'min_child_samples': 16, 'subsample_freq': 4}. Best is trial 0 with value: 0.7246469833119383.\n[I 2023-06-17 13:34:13,703] Trial 4 finished with value: 0.5667522464698331 and parameters: {'reg_alpha': 3.817742738696571e-06, 'reg_lambda': 9.259064360859238e-06, 'num_leaves': 159, 'learning_rate': 0.014244613037565787, 'max_depth': 34, 'colsample_bytree': 0.09660950222198374, 'subsample': 0.7569510367909991, 'min_child_samples': 8, 'subsample_freq': 4}. Best is trial 0 with value: 0.7246469833119383.\n[I 2023-06-17 13:34:13,867] Trial 5 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 5.118465198769174e-06, 'reg_lambda': 9.878546242518675e-06, 'num_leaves': 271, 'learning_rate': 0.06886779418441034, 'max_depth': 22, 'colsample_bytree': 0.03610541389028709, 'subsample': 0.7043411890979814, 'min_child_samples': 14, 'subsample_freq': 4}. Best is trial 5 with value: 0.7528883183568679.\n[I 2023-06-17 13:34:14,228] Trial 6 finished with value: 0.6880616174582799 and parameters: {'reg_alpha': 1.1393237505370685e-06, 'reg_lambda': 2.8290134609524352e-06, 'num_leaves': 231, 'learning_rate': 0.05428959460075924, 'max_depth': 19, 'colsample_bytree': 0.09701760484822311, 'subsample': 0.747610443625731, 'min_child_samples': 1, 'subsample_freq': 1}. Best is trial 5 with value: 0.7528883183568679.\n[I 2023-06-17 13:34:14,360] Trial 7 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 4.293252027830692e-06, 'reg_lambda': 6.003048962738638e-06, 'num_leaves': 204, 'learning_rate': 0.08577091924086043, 'max_depth': 32, 'colsample_bytree': 0.011906863234003065, 'subsample': 0.5313684227695668, 'min_child_samples': 12, 'subsample_freq': 4}. Best is trial 5 with value: 0.7528883183568679.\n[I 2023-06-17 13:34:14,533] Trial 8 finished with value: 0.607188703465982 and parameters: {'reg_alpha': 8.547713460570772e-06, 'reg_lambda': 1.786423399920629e-06, 'num_leaves': 234, 'learning_rate': 0.02004922688689454, 'max_depth': 33, 'colsample_bytree': 0.058618875197461874, 'subsample': 0.9347175747125706, 'min_child_samples': 13, 'subsample_freq': 3}. Best is trial 5 with value: 0.7528883183568679.\n[I 2023-06-17 13:34:14,640] Trial 9 finished with value: 0.6335044929396663 and parameters: {'reg_alpha': 3.435893063029137e-06, 'reg_lambda': 8.037712268634104e-06, 'num_leaves': 247, 'learning_rate': 0.037446162368450546, 'max_depth': 48, 'colsample_bytree': 0.09121749567040516, 'subsample': 0.12181927425113656, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 5 with value: 0.7528883183568679.\n[I 2023-06-17 13:34:14,913] Trial 10 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 9.4849667628316e-06, 'reg_lambda': 9.780046753783228e-06, 'num_leaves': 297, 'learning_rate': 0.07282585402426092, 'max_depth': 11, 'colsample_bytree': 0.046422302231261946, 'subsample': 0.6487837928357725, 'min_child_samples': 7, 'subsample_freq': 6}. Best is trial 5 with value: 0.7528883183568679.\n[I 2023-06-17 13:34:15,170] Trial 11 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 5.816411698360145e-06, 'reg_lambda': 5.7538623436793445e-06, 'num_leaves': 279, 'learning_rate': 0.07619866025883058, 'max_depth': 20, 'colsample_bytree': 0.01167942674371728, 'subsample': 0.5265652996846099, 'min_child_samples': 15, 'subsample_freq': 2}. Best is trial 5 with value: 0.7528883183568679.\n[I 2023-06-17 13:34:15,468] Trial 12 finished with value: 0.7650834403080873 and parameters: {'reg_alpha': 6.69335428240185e-06, 'reg_lambda': 9.74374601120624e-06, 'num_leaves': 206, 'learning_rate': 0.09288693970361951, 'max_depth': 22, 'colsample_bytree': 0.011859147818738178, 'subsample': 0.5247167772372816, 'min_child_samples': 9, 'subsample_freq': 6}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:15,787] Trial 13 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 6.953964103105656e-06, 'reg_lambda': 9.943306076039423e-06, 'num_leaves': 273, 'learning_rate': 0.09971687673934809, 'max_depth': 12, 'colsample_bytree': 0.031348457122249, 'subsample': 0.4304307793124693, 'min_child_samples': 8, 'subsample_freq': 6}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:16,177] Trial 14 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 7.128518019355255e-06, 'reg_lambda': 8.5714444133294e-06, 'num_leaves': 209, 'learning_rate': 0.0673232134099921, 'max_depth': 20, 'colsample_bytree': 0.04568704849623296, 'subsample': 0.6680358099866621, 'min_child_samples': 5, 'subsample_freq': 6}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:16,491] Trial 15 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 5.747667714861605e-06, 'reg_lambda': 9.999110083075624e-06, 'num_leaves': 298, 'learning_rate': 0.06144670000469643, 'max_depth': 25, 'colsample_bytree': 0.020338447665204075, 'subsample': 0.8302928885008847, 'min_child_samples': 16, 'subsample_freq': 5}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:16,735] Trial 16 finished with value: 0.6861360718870345 and parameters: {'reg_alpha': 8.188206901810313e-06, 'reg_lambda': 8.060626898225589e-06, 'num_leaves': 263, 'learning_rate': 0.08162978250523437, 'max_depth': 17, 'colsample_bytree': 0.03395671958172401, 'subsample': 0.5939923190174299, 'min_child_samples': 10, 'subsample_freq': 7}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:17,163] Trial 17 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 2.4745894252419305e-06, 'reg_lambda': 7.33362961621954e-06, 'num_leaves': 218, 'learning_rate': 0.09809899072602143, 'max_depth': 26, 'colsample_bytree': 0.0101507540686873, 'subsample': 0.8370272435969148, 'min_child_samples': 10, 'subsample_freq': 2}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:17,470] Trial 18 finished with value: 0.7002567394094994 and parameters: {'reg_alpha': 4.7784032890908e-06, 'reg_lambda': 8.897264705814403e-06, 'num_leaves': 191, 'learning_rate': 0.06707970282419422, 'max_depth': 15, 'colsample_bytree': 0.022315588778219743, 'subsample': 0.4454963337837899, 'min_child_samples': 14, 'subsample_freq': 5}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:17,745] Trial 19 finished with value: 0.7002567394094994 and parameters: {'reg_alpha': 6.262784500821317e-06, 'reg_lambda': 2.474053822659266e-07, 'num_leaves': 243, 'learning_rate': 0.08939467295524635, 'max_depth': 24, 'colsample_bytree': 0.038875254943064666, 'subsample': 0.6372633172864225, 'min_child_samples': 18, 'subsample_freq': 7}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:18,023] Trial 20 finished with value: 0.6598202824133504 and parameters: {'reg_alpha': 4.62790420022871e-06, 'reg_lambda': 9.320443540040562e-06, 'num_leaves': 151, 'learning_rate': 0.07832923274256272, 'max_depth': 40, 'colsample_bytree': 0.060983252696281035, 'subsample': 0.3392734212121398, 'min_child_samples': 5, 'subsample_freq': 3}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:18,269] Trial 21 finished with value: 0.7548138639281129 and parameters: {'reg_alpha': 6.725748433215042e-06, 'reg_lambda': 9.752542949146493e-06, 'num_leaves': 272, 'learning_rate': 0.09911566128935745, 'max_depth': 14, 'colsample_bytree': 0.025657074287072073, 'subsample': 0.46671673837583094, 'min_child_samples': 8, 'subsample_freq': 7}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:18,520] Trial 22 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 6.386977603325753e-06, 'reg_lambda': 8.95361428998238e-06, 'num_leaves': 285, 'learning_rate': 0.09414693504949792, 'max_depth': 14, 'colsample_bytree': 0.019000538432225572, 'subsample': 0.5504483876697461, 'min_child_samples': 11, 'subsample_freq': 7}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:18,741] Trial 23 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 5.056908496160793e-06, 'reg_lambda': 7.92202812105847e-06, 'num_leaves': 253, 'learning_rate': 0.0887606273064627, 'max_depth': 21, 'colsample_bytree': 0.026100684736293484, 'subsample': 0.4862726819745633, 'min_child_samples': 9, 'subsample_freq': 8}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:19,036] Trial 24 finished with value: 0.7002567394094994 and parameters: {'reg_alpha': 7.792629766067361e-06, 'reg_lambda': 9.936907858420925e-06, 'num_leaves': 269, 'learning_rate': 0.09962056040645303, 'max_depth': 10, 'colsample_bytree': 0.018475588938629158, 'subsample': 0.3688153779372023, 'min_child_samples': 6, 'subsample_freq': 5}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:19,343] Trial 25 finished with value: 0.6861360718870345 and parameters: {'reg_alpha': 6.074322005822748e-06, 'reg_lambda': 9.074499835315332e-06, 'num_leaves': 224, 'learning_rate': 0.08357944393324926, 'max_depth': 17, 'colsample_bytree': 0.039445355151518924, 'subsample': 0.60200877111092, 'min_child_samples': 11, 'subsample_freq': 7}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:19,669] Trial 26 finished with value: 0.7002567394094994 and parameters: {'reg_alpha': 5.359879666220161e-06, 'reg_lambda': 7.784288501310642e-06, 'num_leaves': 286, 'learning_rate': 0.09274121994744042, 'max_depth': 28, 'colsample_bytree': 0.027667473773856817, 'subsample': 0.4913948622152159, 'min_child_samples': 4, 'subsample_freq': 9}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:20,020] Trial 27 finished with value: 0.7002567394094994 and parameters: {'reg_alpha': 7.781349139299146e-06, 'reg_lambda': 8.589688421073841e-06, 'num_leaves': 191, 'learning_rate': 0.08411876647774123, 'max_depth': 22, 'colsample_bytree': 0.01759890745025341, 'subsample': 0.6971648416911991, 'min_child_samples': 8, 'subsample_freq': 6}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:20,299] Trial 28 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 9.396013836606607e-06, 'reg_lambda': 9.418010190327862e-06, 'num_leaves': 236, 'learning_rate': 0.07722851640995727, 'max_depth': 15, 'colsample_bytree': 0.03385857568006434, 'subsample': 0.5478879610891895, 'min_child_samples': 13, 'subsample_freq': 3}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:20,532] Trial 29 finished with value: 0.7650834403080873 and parameters: {'reg_alpha': 6.493645831303037e-06, 'reg_lambda': 7.25038279414387e-06, 'num_leaves': 264, 'learning_rate': 0.09137729672464998, 'max_depth': 37, 'colsample_bytree': 0.028534227900319654, 'subsample': 0.32912352782141197, 'min_child_samples': 17, 'subsample_freq': 9}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:20,783] Trial 30 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 6.547920495520985e-06, 'reg_lambda': 7.115213320229354e-06, 'num_leaves': 260, 'learning_rate': 0.09249284890138831, 'max_depth': 39, 'colsample_bytree': 0.025107747310274167, 'subsample': 0.3014677072562971, 'min_child_samples': 19, 'subsample_freq': 10}. Best is trial 12 with value: 0.7650834403080873.\n[I 2023-06-17 13:34:20,993] Trial 31 finished with value: 0.803594351732991 and parameters: {'reg_alpha': 5.466722266660606e-06, 'reg_lambda': 9.368741367548251e-06, 'num_leaves': 269, 'learning_rate': 0.08910079912479407, 'max_depth': 37, 'colsample_bytree': 0.029629249652321058, 'subsample': 0.2724619458652722, 'min_child_samples': 17, 'subsample_freq': 9}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:21,369] Trial 32 finished with value: 0.7246469833119383 and parameters: {'reg_alpha': 7.068499799643547e-06, 'reg_lambda': 8.52375728658937e-06, 'num_leaves': 286, 'learning_rate': 0.09270049521821348, 'max_depth': 37, 'colsample_bytree': 0.029807263631649886, 'subsample': 0.2539346129786737, 'min_child_samples': 17, 'subsample_freq': 9}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:21,689] Trial 33 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 6.653956626237971e-06, 'reg_lambda': 9.190729180144734e-06, 'num_leaves': 248, 'learning_rate': 0.08963318005409251, 'max_depth': 44, 'colsample_bytree': 0.0163478625124283, 'subsample': 0.38467106196945394, 'min_child_samples': 19, 'subsample_freq': 8}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:22,227] Trial 34 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 6.003717997341686e-06, 'reg_lambda': 6.637393423228901e-06, 'num_leaves': 261, 'learning_rate': 0.0956957222276708, 'max_depth': 44, 'colsample_bytree': 0.026763691997154303, 'subsample': 0.21313532808462127, 'min_child_samples': 17, 'subsample_freq': 9}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:22,594] Trial 35 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 5.43017556984322e-06, 'reg_lambda': 7.510393787881908e-06, 'num_leaves': 174, 'learning_rate': 0.08667544666408637, 'max_depth': 36, 'colsample_bytree': 0.02253940155685587, 'subsample': 0.40761038074927625, 'min_child_samples': 20, 'subsample_freq': 8}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:22,962] Trial 36 finished with value: 0.7792041078305519 and parameters: {'reg_alpha': 7.368486091313521e-06, 'reg_lambda': 8.325882739364152e-06, 'num_leaves': 218, 'learning_rate': 0.08157548948090343, 'max_depth': 31, 'colsample_bytree': 0.015510165636043089, 'subsample': 0.3466047198201009, 'min_child_samples': 9, 'subsample_freq': 10}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:23,377] Trial 37 finished with value: 0.6880616174582799 and parameters: {'reg_alpha': 7.388601136588045e-06, 'reg_lambda': 8.230951253482362e-06, 'num_leaves': 215, 'learning_rate': 0.0822990830013581, 'max_depth': 31, 'colsample_bytree': 0.014393163243459236, 'subsample': 0.3170396659177778, 'min_child_samples': 2, 'subsample_freq': 10}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:23,627] Trial 38 finished with value: 0.7509627727856225 and parameters: {'reg_alpha': 7.511244273375415e-06, 'reg_lambda': 7.4529702079042836e-06, 'num_leaves': 198, 'learning_rate': 0.0799416477151411, 'max_depth': 29, 'colsample_bytree': 0.013773994918027714, 'subsample': 0.1892157214162785, 'min_child_samples': 15, 'subsample_freq': 10}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:23,949] Trial 39 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 8.818679845974143e-06, 'reg_lambda': 8.628826233245164e-06, 'num_leaves': 223, 'learning_rate': 0.07345776834786186, 'max_depth': 35, 'colsample_bytree': 0.010234270415890472, 'subsample': 0.2777964384677663, 'min_child_samples': 9, 'subsample_freq': 9}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:24,316] Trial 40 finished with value: 0.7913992297817715 and parameters: {'reg_alpha': 5.503498466927932e-06, 'reg_lambda': 6.873412339356124e-06, 'num_leaves': 211, 'learning_rate': 0.0871267131600589, 'max_depth': 42, 'colsample_bytree': 0.020495949250849993, 'subsample': 0.35879280785624285, 'min_child_samples': 12, 'subsample_freq': 8}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:24,623] Trial 41 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 6.018149859879682e-06, 'reg_lambda': 6.596853239447021e-06, 'num_leaves': 186, 'learning_rate': 0.08696508553436944, 'max_depth': 43, 'colsample_bytree': 0.016581773452411484, 'subsample': 0.3469633480278657, 'min_child_samples': 12, 'subsample_freq': 8}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:24,875] Trial 42 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 5.257528353001609e-06, 'reg_lambda': 6.971666732961315e-06, 'num_leaves': 211, 'learning_rate': 0.09192941600896094, 'max_depth': 50, 'colsample_bytree': 0.01931179337734003, 'subsample': 0.38581905800233984, 'min_child_samples': 17, 'subsample_freq': 10}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:25,121] Trial 43 finished with value: 0.7650834403080873 and parameters: {'reg_alpha': 4.409206216257303e-06, 'reg_lambda': 8.129028799669609e-06, 'num_leaves': 200, 'learning_rate': 0.08703426360654955, 'max_depth': 38, 'colsample_bytree': 0.02220650935851224, 'subsample': 0.300354751938199, 'min_child_samples': 15, 'subsample_freq': 9}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:25,393] Trial 44 finished with value: 0.6335044929396663 and parameters: {'reg_alpha': 5.633893780807271e-06, 'reg_lambda': 6.111951056372995e-06, 'num_leaves': 230, 'learning_rate': 0.09608586961750562, 'max_depth': 33, 'colsample_bytree': 0.015000138891773221, 'subsample': 0.4195814409958847, 'min_child_samples': 7, 'subsample_freq': 9}. Best is trial 31 with value: 0.803594351732991.\n[I 2023-06-17 13:34:25,626] Trial 45 finished with value: 0.8177150192554556 and parameters: {'reg_alpha': 6.833177329036985e-06, 'reg_lambda': 4.958487110482849e-06, 'num_leaves': 241, 'learning_rate': 0.08139171493009001, 'max_depth': 42, 'colsample_bytree': 0.032120190527832874, 'subsample': 0.2178632721062016, 'min_child_samples': 13, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:25,850] Trial 46 finished with value: 0.6861360718870345 and parameters: {'reg_alpha': 7.038037987216774e-06, 'reg_lambda': 4.7911873670578926e-06, 'num_leaves': 240, 'learning_rate': 0.08131293697090068, 'max_depth': 41, 'colsample_bytree': 0.031802377254478195, 'subsample': 0.17290031554397695, 'min_child_samples': 13, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:26,089] Trial 47 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 5.6457552124640885e-06, 'reg_lambda': 5.55199619086776e-06, 'num_leaves': 170, 'learning_rate': 0.07459939789965549, 'max_depth': 46, 'colsample_bytree': 0.02296107963848295, 'subsample': 0.2615735963006692, 'min_child_samples': 9, 'subsample_freq': 8}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:26,328] Trial 48 finished with value: 0.7913992297817715 and parameters: {'reg_alpha': 4.1861800647397456e-06, 'reg_lambda': 4.023977409676484e-06, 'num_leaves': 218, 'learning_rate': 0.0706396693917404, 'max_depth': 42, 'colsample_bytree': 0.014229865187637207, 'subsample': 0.22393357539941627, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:26,541] Trial 49 finished with value: 0.67201540436457 and parameters: {'reg_alpha': 3.910713544720314e-06, 'reg_lambda': 4.27447445066336e-06, 'num_leaves': 228, 'learning_rate': 0.07110659396501755, 'max_depth': 47, 'colsample_bytree': 0.01408413399504038, 'subsample': 0.10730180445304276, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:26,780] Trial 50 finished with value: 0.7650834403080873 and parameters: {'reg_alpha': 4.903139029468073e-06, 'reg_lambda': 4.180964087613467e-06, 'num_leaves': 218, 'learning_rate': 0.07778698777857929, 'max_depth': 42, 'colsample_bytree': 0.020817516177103736, 'subsample': 0.22654113872738554, 'min_child_samples': 14, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:27,163] Trial 51 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 4.1815920844525684e-06, 'reg_lambda': 5.163266357620727e-06, 'num_leaves': 206, 'learning_rate': 0.08430589206918249, 'max_depth': 45, 'colsample_bytree': 0.010242820568928597, 'subsample': 0.16231426614989067, 'min_child_samples': 11, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:27,572] Trial 52 finished with value: 0.6983311938382541 and parameters: {'reg_alpha': 6.248425462024658e-06, 'reg_lambda': 3.6511537345323593e-06, 'num_leaves': 212, 'learning_rate': 0.07931088039121094, 'max_depth': 40, 'colsample_bytree': 0.014000476282206582, 'subsample': 0.24338514147331108, 'min_child_samples': 10, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:27,875] Trial 53 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 6.7450474527517294e-06, 'reg_lambda': 9.615659491562484e-06, 'num_leaves': 221, 'learning_rate': 0.08497587981309207, 'max_depth': 42, 'colsample_bytree': 0.01841747099583769, 'subsample': 0.28314463940873, 'min_child_samples': 13, 'subsample_freq': 8}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:28,220] Trial 54 finished with value: 0.6983311938382541 and parameters: {'reg_alpha': 4.975630581861971e-06, 'reg_lambda': 9.542221781376732e-06, 'num_leaves': 200, 'learning_rate': 0.06978084271856716, 'max_depth': 34, 'colsample_bytree': 0.023814181173274097, 'subsample': 0.1512436733806693, 'min_child_samples': 7, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:28,599] Trial 55 finished with value: 0.7792041078305519 and parameters: {'reg_alpha': 3.3304734512029055e-06, 'reg_lambda': 8.947134469005305e-06, 'num_leaves': 235, 'learning_rate': 0.07523170507184522, 'max_depth': 31, 'colsample_bytree': 0.01267343206942688, 'subsample': 0.222434007139356, 'min_child_samples': 10, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:28,833] Trial 56 finished with value: 0.7913992297817715 and parameters: {'reg_alpha': 3.5269910466663763e-06, 'reg_lambda': 8.917639415544314e-06, 'num_leaves': 234, 'learning_rate': 0.07500809247443548, 'max_depth': 30, 'colsample_bytree': 0.030935845287178042, 'subsample': 0.23753319965265177, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:29,064] Trial 57 finished with value: 0.7913992297817715 and parameters: {'reg_alpha': 3.682325912445872e-06, 'reg_lambda': 7.676518277830257e-06, 'num_leaves': 252, 'learning_rate': 0.06272663655341268, 'max_depth': 27, 'colsample_bytree': 0.030569892556229546, 'subsample': 0.2145873490475823, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:29,262] Trial 58 finished with value: 0.6456996148908858 and parameters: {'reg_alpha': 2.853345660570886e-06, 'reg_lambda': 7.821884878631593e-06, 'num_leaves': 253, 'learning_rate': 0.06388624642731093, 'max_depth': 27, 'colsample_bytree': 0.036129056527159115, 'subsample': 0.14415789443230226, 'min_child_samples': 14, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:29,510] Trial 59 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 4.483711199056706e-06, 'reg_lambda': 6.157321303495562e-06, 'num_leaves': 247, 'learning_rate': 0.055801751223226746, 'max_depth': 48, 'colsample_bytree': 0.029618865236394697, 'subsample': 0.19813658770884865, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:29,722] Trial 60 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 3.865205026368242e-06, 'reg_lambda': 6.650822966527391e-06, 'num_leaves': 242, 'learning_rate': 0.07095236380781746, 'max_depth': 24, 'colsample_bytree': 0.04229492696718241, 'subsample': 0.2334985599829437, 'min_child_samples': 16, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:29,963] Trial 61 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 3.601228784321972e-06, 'reg_lambda': 8.389755567725043e-06, 'num_leaves': 228, 'learning_rate': 0.07537366948543428, 'max_depth': 30, 'colsample_bytree': 0.03199432130916533, 'subsample': 0.27061282419326127, 'min_child_samples': 11, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:30,174] Trial 62 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 4.68769145350399e-06, 'reg_lambda': 8.745858999585013e-06, 'num_leaves': 255, 'learning_rate': 0.06629864473936399, 'max_depth': 39, 'colsample_bytree': 0.02648286216827223, 'subsample': 0.18680660890479703, 'min_child_samples': 14, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:30,394] Trial 63 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 3.1188305916920367e-06, 'reg_lambda': 7.714464546725388e-06, 'num_leaves': 232, 'learning_rate': 0.07948345601288588, 'max_depth': 32, 'colsample_bytree': 0.03628707638728464, 'subsample': 0.3466131114871879, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:30,624] Trial 64 finished with value: 0.6983311938382541 and parameters: {'reg_alpha': 4.177402494961643e-06, 'reg_lambda': 9.151639365423851e-06, 'num_leaves': 237, 'learning_rate': 0.07346311139422722, 'max_depth': 28, 'colsample_bytree': 0.02865637249109588, 'subsample': 0.2030531805223943, 'min_child_samples': 13, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:30,886] Trial 65 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 5.11823615457713e-06, 'reg_lambda': 8.188668807292143e-06, 'num_leaves': 250, 'learning_rate': 0.08213679300948462, 'max_depth': 36, 'colsample_bytree': 0.024399543789586482, 'subsample': 0.30018551039317265, 'min_child_samples': 10, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:31,137] Trial 66 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 2.2182070931022907e-06, 'reg_lambda': 8.809679736808705e-06, 'num_leaves': 278, 'learning_rate': 0.060730532490875545, 'max_depth': 25, 'colsample_bytree': 0.020853314057234392, 'subsample': 0.24291640548773896, 'min_child_samples': 15, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:31,433] Trial 67 finished with value: 0.6739409499358152 and parameters: {'reg_alpha': 4.770436541285804e-06, 'reg_lambda': 9.35292058832319e-06, 'num_leaves': 267, 'learning_rate': 0.07690655249312715, 'max_depth': 41, 'colsample_bytree': 0.033388625503934885, 'subsample': 0.12592087961770887, 'min_child_samples': 11, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:31,714] Trial 68 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 4.160945048939908e-06, 'reg_lambda': 7.537182300346973e-06, 'num_leaves': 219, 'learning_rate': 0.08837634128616055, 'max_depth': 38, 'colsample_bytree': 0.03008010801608094, 'subsample': 0.2673090305742272, 'min_child_samples': 12, 'subsample_freq': 8}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:32,007] Trial 69 finished with value: 0.7913992297817715 and parameters: {'reg_alpha': 3.6047904549948987e-06, 'reg_lambda': 8.341226736206467e-06, 'num_leaves': 245, 'learning_rate': 0.06839308680378803, 'max_depth': 33, 'colsample_bytree': 0.01680383527677016, 'subsample': 0.3242310531426571, 'min_child_samples': 13, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:32,293] Trial 70 finished with value: 0.6861360718870345 and parameters: {'reg_alpha': 3.5898175944470653e-06, 'reg_lambda': 7.8551664421891e-06, 'num_leaves': 242, 'learning_rate': 0.06819266691813823, 'max_depth': 43, 'colsample_bytree': 0.020161614050913598, 'subsample': 0.21357321537340776, 'min_child_samples': 13, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:32,828] Trial 71 finished with value: 0.7650834403080873 and parameters: {'reg_alpha': 4.572734862275405e-06, 'reg_lambda': 8.37373958808928e-06, 'num_leaves': 246, 'learning_rate': 0.07213056658823405, 'max_depth': 34, 'colsample_bytree': 0.025049360843253737, 'subsample': 0.30823099660945885, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:33,212] Trial 72 finished with value: 0.6861360718870345 and parameters: {'reg_alpha': 3.910743227137148e-06, 'reg_lambda': 8.086556477691357e-06, 'num_leaves': 257, 'learning_rate': 0.06517137575014685, 'max_depth': 30, 'colsample_bytree': 0.01714591544314907, 'subsample': 0.36303050166566564, 'min_child_samples': 13, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:33,534] Trial 73 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 5.337059326074386e-06, 'reg_lambda': 8.715914323793236e-06, 'num_leaves': 225, 'learning_rate': 0.07072140293683056, 'max_depth': 32, 'colsample_bytree': 0.016757300385110856, 'subsample': 0.24760387386533406, 'min_child_samples': 11, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:33,815] Trial 74 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 3.0664381074209104e-06, 'reg_lambda': 9.055752281170607e-06, 'num_leaves': 215, 'learning_rate': 0.06836275742023559, 'max_depth': 27, 'colsample_bytree': 0.027325785359875156, 'subsample': 0.32220011489608785, 'min_child_samples': 14, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:34,067] Trial 75 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 3.5430401420861563e-06, 'reg_lambda': 7.215250245336369e-06, 'num_leaves': 232, 'learning_rate': 0.08036433759913221, 'max_depth': 29, 'colsample_bytree': 0.022699664336450827, 'subsample': 0.2902726071356899, 'min_child_samples': 14, 'subsample_freq': 1}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:34,318] Trial 76 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 4.369190457963379e-06, 'reg_lambda': 8.403882704422792e-06, 'num_leaves': 239, 'learning_rate': 0.08466923405846477, 'max_depth': 35, 'colsample_bytree': 0.012031616268681112, 'subsample': 0.3292460963143919, 'min_child_samples': 16, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:34,584] Trial 77 finished with value: 0.7124518613607188 and parameters: {'reg_alpha': 6.274107487125823e-06, 'reg_lambda': 9.403758428604548e-06, 'num_leaves': 274, 'learning_rate': 0.07705730311090644, 'max_depth': 33, 'colsample_bytree': 0.01856994565716151, 'subsample': 0.1848861662182153, 'min_child_samples': 8, 'subsample_freq': 8}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:34,897] Trial 78 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 5.846823084357077e-06, 'reg_lambda': 7.996378890914665e-06, 'num_leaves': 259, 'learning_rate': 0.08257868253677735, 'max_depth': 37, 'colsample_bytree': 0.020711552173137644, 'subsample': 0.25744411177322535, 'min_child_samples': 11, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:35,274] Trial 79 finished with value: 0.6861360718870345 and parameters: {'reg_alpha': 5.531376044298831e-06, 'reg_lambda': 7.648450400326647e-06, 'num_leaves': 225, 'learning_rate': 0.07425746818663381, 'max_depth': 31, 'colsample_bytree': 0.015317837312024638, 'subsample': 0.2811766452492479, 'min_child_samples': 9, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:35,722] Trial 80 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 5.082047229725197e-06, 'reg_lambda': 9.742796849557028e-06, 'num_leaves': 208, 'learning_rate': 0.08609623629816102, 'max_depth': 26, 'colsample_bytree': 0.02786763060174598, 'subsample': 0.3959653863334006, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:36,036] Trial 81 finished with value: 0.7913992297817715 and parameters: {'reg_alpha': 3.500159268362449e-06, 'reg_lambda': 9.115067724267246e-06, 'num_leaves': 235, 'learning_rate': 0.07592141083815848, 'max_depth': 31, 'colsample_bytree': 0.013357326811004298, 'subsample': 0.21438257939736807, 'min_child_samples': 10, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:36,307] Trial 82 finished with value: 0.7792041078305519 and parameters: {'reg_alpha': 3.7457007169003016e-06, 'reg_lambda': 8.937884178796169e-06, 'num_leaves': 244, 'learning_rate': 0.08918323385077676, 'max_depth': 29, 'colsample_bytree': 0.015072901809256647, 'subsample': 0.2219900862344286, 'min_child_samples': 10, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:36,595] Trial 83 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 3.3602550013310547e-06, 'reg_lambda': 8.545462951682e-06, 'num_leaves': 293, 'learning_rate': 0.0775412789248137, 'max_depth': 45, 'colsample_bytree': 0.012284907515173221, 'subsample': 0.19927002921060624, 'min_child_samples': 13, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:36,989] Trial 84 finished with value: 0.738767650834403 and parameters: {'reg_alpha': 4.082745907322114e-06, 'reg_lambda': 9.204840510574754e-06, 'num_leaves': 215, 'learning_rate': 0.08070929963564594, 'max_depth': 32, 'colsample_bytree': 0.017566684153219415, 'subsample': 0.16353489495542672, 'min_child_samples': 11, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:37,390] Trial 85 finished with value: 0.6739409499358152 and parameters: {'reg_alpha': 4.340843198622184e-06, 'reg_lambda': 8.276175044091635e-06, 'num_leaves': 251, 'learning_rate': 0.07263778399320646, 'max_depth': 35, 'colsample_bytree': 0.025077133667912213, 'subsample': 0.24463370534758005, 'min_child_samples': 10, 'subsample_freq': 7}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:37,780] Trial 86 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 2.8037577199566502e-06, 'reg_lambda': 8.782133686340415e-06, 'num_leaves': 236, 'learning_rate': 0.06934938204122297, 'max_depth': 40, 'colsample_bytree': 0.023629370061241474, 'subsample': 0.362804628654643, 'min_child_samples': 9, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:38,053] Trial 87 finished with value: 0.7246469833119383 and parameters: {'reg_alpha': 3.6878170211727976e-06, 'reg_lambda': 7.402271586911163e-06, 'num_leaves': 264, 'learning_rate': 0.07883380945713979, 'max_depth': 28, 'colsample_bytree': 0.010393414666074945, 'subsample': 0.316026375167769, 'min_child_samples': 18, 'subsample_freq': 8}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:38,413] Trial 88 finished with value: 0.7772785622593068 and parameters: {'reg_alpha': 6.799517092724115e-06, 'reg_lambda': 9.968687528233445e-06, 'num_leaves': 227, 'learning_rate': 0.08288345272976473, 'max_depth': 34, 'colsample_bytree': 0.021187001035922953, 'subsample': 0.2881305341505198, 'min_child_samples': 15, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:38,723] Trial 89 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 4.658281010349448e-06, 'reg_lambda': 9.511631054794294e-06, 'num_leaves': 195, 'learning_rate': 0.07576653721764243, 'max_depth': 39, 'colsample_bytree': 0.018823219202243058, 'subsample': 0.17542264035734328, 'min_child_samples': 6, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:39,077] Trial 90 finished with value: 0.7772785622593068 and parameters: {'reg_alpha': 7.3099449514776506e-06, 'reg_lambda': 8.025663547934411e-06, 'num_leaves': 231, 'learning_rate': 0.09078027682230941, 'max_depth': 42, 'colsample_bytree': 0.015695452306885026, 'subsample': 0.22589621116247366, 'min_child_samples': 12, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:39,430] Trial 91 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 3.25255532291131e-06, 'reg_lambda': 8.966007289540044e-06, 'num_leaves': 236, 'learning_rate': 0.07404421552051671, 'max_depth': 31, 'colsample_bytree': 0.013520049900379996, 'subsample': 0.2640984594680291, 'min_child_samples': 8, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:39,703] Trial 92 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 3.419790940235944e-06, 'reg_lambda': 8.53282485666143e-06, 'num_leaves': 221, 'learning_rate': 0.07187455881396201, 'max_depth': 30, 'colsample_bytree': 0.0137355702752298, 'subsample': 0.23192910587123355, 'min_child_samples': 10, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:39,976] Trial 93 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 3.967894128869646e-06, 'reg_lambda': 9.319090790476035e-06, 'num_leaves': 234, 'learning_rate': 0.0762392434542125, 'max_depth': 33, 'colsample_bytree': 0.012414838013362973, 'subsample': 0.20986708229928838, 'min_child_samples': 9, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:40,210] Trial 94 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 4.02703440566466e-06, 'reg_lambda': 8.98735912487149e-06, 'num_leaves': 239, 'learning_rate': 0.07854932460090203, 'max_depth': 31, 'colsample_bytree': 0.016521495289736125, 'subsample': 0.1318107852172427, 'min_child_samples': 11, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:40,534] Trial 95 finished with value: 0.67201540436457 and parameters: {'reg_alpha': 3.7538509157965855e-06, 'reg_lambda': 8.667786630002935e-06, 'num_leaves': 249, 'learning_rate': 0.07503709431938478, 'max_depth': 27, 'colsample_bytree': 0.01995842838856971, 'subsample': 0.1525127050409511, 'min_child_samples': 10, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:40,844] Trial 96 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 3.4454926629534653e-06, 'reg_lambda': 9.698369073949953e-06, 'num_leaves': 245, 'learning_rate': 0.08120556190831955, 'max_depth': 36, 'colsample_bytree': 0.03160221205085742, 'subsample': 0.34098054228531327, 'min_child_samples': 13, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:41,097] Trial 97 finished with value: 0.7528883183568679 and parameters: {'reg_alpha': 4.8698240752875666e-06, 'reg_lambda': 8.235088959163813e-06, 'num_leaves': 228, 'learning_rate': 0.08431429970145601, 'max_depth': 24, 'colsample_bytree': 0.01080518683571033, 'subsample': 0.2724648548002152, 'min_child_samples': 12, 'subsample_freq': 10}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:41,348] Trial 98 finished with value: 0.6861360718870345 and parameters: {'reg_alpha': 4.479433764818269e-06, 'reg_lambda': 9.25077855333062e-06, 'num_leaves': 212, 'learning_rate': 0.08721441190446041, 'max_depth': 30, 'colsample_bytree': 0.012724977309432514, 'subsample': 0.1842297494251861, 'min_child_samples': 10, 'subsample_freq': 9}. Best is trial 45 with value: 0.8177150192554556.\n[I 2023-06-17 13:34:41,592] Trial 99 finished with value: 0.7265725288831836 and parameters: {'reg_alpha': 5.828726264747135e-06, 'reg_lambda': 7.86110189452856e-06, 'num_leaves': 202, 'learning_rate': 0.0699043960988844, 'max_depth': 43, 'colsample_bytree': 0.02260743022163131, 'subsample': 0.21087651740497554, 'min_child_samples': 11, 'subsample_freq': 4}. Best is trial 45 with value: 0.8177150192554556.\n\n\nCPU times: total: 2min 25s\nWall time: 29.1 s"
  },
  {
    "objectID": "posts/Optimization_with_Optuna/index.html#quick-visualization-for-hyperparameter-optimization-analysis",
    "href": "posts/Optimization_with_Optuna/index.html#quick-visualization-for-hyperparameter-optimization-analysis",
    "title": "Hyperparameter Optimization with Optuna",
    "section": "Quick Visualization for Hyperparameter Optimization Analysis",
    "text": "Quick Visualization for Hyperparameter Optimization Analysis\n\n# Get the best set of hyperparameters\n\noptimized_params = study.best_trial.params\noptimized_params\n\n{'reg_alpha': 6.833177329036985e-06,\n 'reg_lambda': 4.958487110482849e-06,\n 'num_leaves': 241,\n 'learning_rate': 0.08139171493009001,\n 'max_depth': 42,\n 'colsample_bytree': 0.032120190527832874,\n 'subsample': 0.2178632721062016,\n 'min_child_samples': 13,\n 'subsample_freq': 10}\n\n\n\n# plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\noptuna.visualization.plot_slice(study)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "posts/Optimization_with_Optuna/index.html#lgbm-with-optimized-parameter",
    "href": "posts/Optimization_with_Optuna/index.html#lgbm-with-optimized-parameter",
    "title": "Hyperparameter Optimization with Optuna",
    "section": "LGBM with Optimized Parameter",
    "text": "LGBM with Optimized Parameter\n\n## Additional params\noptimized_params['objective'] = 'binary'\noptimized_params['metric'] = 'binary_logloss'\noptimized_params['n_estimators'] = 200\noptimized_params['random_state'] = 23\n\n## Updated optimized hyperparameters\noptimized_params\n\n{'reg_alpha': 6.833177329036985e-06,\n 'reg_lambda': 4.958487110482849e-06,\n 'num_leaves': 241,\n 'learning_rate': 0.08139171493009001,\n 'max_depth': 42,\n 'colsample_bytree': 0.032120190527832874,\n 'subsample': 0.2178632721062016,\n 'min_child_samples': 13,\n 'subsample_freq': 10,\n 'objective': 'binary',\n 'metric': 'binary_logloss',\n 'n_estimators': 200,\n 'random_state': 23}\n\n\n\n%%time\nlgbm_model = LGBMClassifier(**optimized_params)\nlgbm_model.fit(X_train, y_train)\ny_predict = lgbm_model.predict(X_test)\n\nCPU times: total: 469 ms\nWall time: 95 ms\n\n\n\nroc_auc_score(y_test, y_predict)\n\n0.8177150192554556\n\n\n\nmetrics = []\ncm = []\n\n\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, auc\nfrom sklearn.metrics import make_scorer, recall_score, precision_score\nfrom sklearn.metrics import roc_curve, precision_recall_curve, matthews_corrcoef\n\n\n## In heart failure, 0 defines survive and 1 represents confirmed death\nspecificity = make_scorer(precision_score, pos_label=0)\nnpv = make_scorer(recall_score, pos_label=0)\n\n\nprecision, recall, fscore, _ = score(y_test, y_predict, average='binary')\naccuracy = accuracy_score(y_test, y_predict)\ntnr = precision_score(y_test, y_predict, pos_label=0, average='binary')\nnpv = recall_score(y_test, y_predict, pos_label=0, average='binary')\nmcc = matthews_corrcoef(y_test, y_predict)\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_predict)\npr = auc(recall_curve, precision_curve)\nroc = roc_auc_score(y_test, y_predict)\n\n\ncm.append(confusion_matrix(y_test, y_predict, labels=[1,0]))\nmetrics.append(pd.Series({'precision':precision,\n                        'recall':recall,\n                        'fscore':fscore,\n                        'specificity':tnr,\n                        'NPV': npv,\n                        'accuracy':accuracy, \n                        'MCC':mcc,\n                        'Precision-Recall AUC':pr,\n                        'ROC AUC':roc}))\n\nmetrics = pd.concat(metrics, axis=1)\n\n\nmetrics\n\n\n\n\n\n\n\n\n0\n\n\n\n\nprecision\n0.866667\n\n\nrecall\n0.684211\n\n\nfscore\n0.764706\n\n\nspecificity\n0.866667\n\n\nNPV\n0.951220\n\n\naccuracy\n0.866667\n\n\nMCC\n0.682629\n\n\nPrecision-Recall AUC\n0.825439\n\n\nROC AUC\n0.817715"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/XGBoost_to_BentoML/index.html",
    "href": "posts/XGBoost_to_BentoML/index.html",
    "title": "Building XGBoost Package with BentoML",
    "section": "",
    "text": "After completing the training of the algorithm on the dataset, the next crucial step is deploying the model to serve predictions within an API interface. However, deploying the model using Flask application poses scalability challenges. When faced with high traffic calls, especially in production environments where hundreds or even thousands of requests may arrive simultaneously, Flask’s WSGI standard hampers its ability to receive new requests until the previous ones are completed. This limitation restricts the service’s capacity to handle parallel requests efficiently.\nTo overcome this challenge, a successor to WSGI called ASGI was introduced. ASGI allows for the handling of multiple requests simultaneously by leveraging its asynchronous behavior. This modern web framework provides improved efficiency while still retaining the essential features of WSGI when required. In response to these advancements, BentoML emerged as a solution.\nBentoML propose functionality that allows data scientists and ML engineers to:\n\nMake transition from experiment in notebook to ML service with scalability in standard way. You can create and package your ML service useful for production.\nCustomize ML service to fit well with specific use cases.\nVerify that ML service is ready for production. This includes deployment, monitoring, and operation.\n\nThis notebook will demonstrate how making XGBoost service package is possible with BentoML.\n\nImporting Libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\n\nLoading Data Credit Scoring\n\ndata = 'CreditScoring.csv'\n\ndf = pd.read_csv(data)\ndf.columns = df.columns.str.lower()\n\n\n\nData Preprocessing\nLet’s create a mapping to modify categorical variables.\n\nstatus_values = {\n    1: 'ok',\n    2: 'default',\n    0: 'unk'\n}\n\ndf.status = df.status.map(status_values)\n\n\nhome_values = {\n    1: 'rent',\n    2: 'owner',\n    3: 'private',\n    4: 'ignore',\n    5: 'parents',\n    6: 'other',\n    0: 'unk'\n}\n\ndf.home = df.home.map(home_values)\n\nmarital_values = {\n    1: 'single',\n    2: 'married',\n    3: 'widow',\n    4: 'separated',\n    5: 'divorced',\n    0: 'unk'\n}\n\ndf.marital = df.marital.map(marital_values)\n\nrecords_values = {\n    1: 'no',\n    2: 'yes',\n    0: 'unk'\n}\n\ndf.records = df.records.map(records_values)\n\njob_values = {\n    1: 'fixed',\n    2: 'partime',\n    3: 'freelance',\n    4: 'others',\n    0: 'unk'\n}\n\ndf.job = df.job.map(job_values)\n\nA simple descriptive distribution on numerical features.\n\ndf.describe().round()\n\n\n\n\n\n\n\n\nseniority\ntime\nage\nexpenses\nincome\nassets\ndebt\namount\nprice\n\n\n\n\ncount\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n4455.0\n\n\nmean\n8.0\n46.0\n37.0\n56.0\n763317.0\n1060341.0\n404382.0\n1039.0\n1463.0\n\n\nstd\n8.0\n15.0\n11.0\n20.0\n8703625.0\n10217569.0\n6344253.0\n475.0\n628.0\n\n\nmin\n0.0\n6.0\n18.0\n35.0\n0.0\n0.0\n0.0\n100.0\n105.0\n\n\n25%\n2.0\n36.0\n28.0\n35.0\n80.0\n0.0\n0.0\n700.0\n1118.0\n\n\n50%\n5.0\n48.0\n36.0\n51.0\n120.0\n3500.0\n0.0\n1000.0\n1400.0\n\n\n75%\n12.0\n60.0\n45.0\n72.0\n166.0\n6000.0\n0.0\n1300.0\n1692.0\n\n\nmax\n48.0\n72.0\n68.0\n180.0\n99999999.0\n99999999.0\n99999999.0\n5000.0\n11140.0\n\n\n\n\n\n\n\nThere are a few extreme values on column income, assets, and debt. We replace them to NaN values.\n\nfor c in ['income', 'assets', 'debt']:\n    df[c] = df[c].replace(to_replace=99999999, value=np.nan)\n\n\ndf.isnull().sum()\n\nstatus        0\nseniority     0\nhome          0\ntime          0\nage           0\nmarital       0\nrecords       0\njob           0\nexpenses      0\nincome       34\nassets       47\ndebt         18\namount        0\nprice         0\ndtype: int64\n\n\n\ndf = df[df.status != 'unk'].reset_index(drop=True)\n\nA more meaningful description.\n\ndf.describe().round()\n\n\n\n\n\n\n\n\nseniority\ntime\nage\nexpenses\nincome\nassets\ndebt\namount\nprice\n\n\n\n\ncount\n4454.0\n4454.0\n4454.0\n4454.0\n4420.0\n4407.0\n4436.0\n4454.0\n4454.0\n\n\nmean\n8.0\n46.0\n37.0\n56.0\n131.0\n5404.0\n343.0\n1039.0\n1463.0\n\n\nstd\n8.0\n15.0\n11.0\n20.0\n86.0\n11574.0\n1246.0\n475.0\n628.0\n\n\nmin\n0.0\n6.0\n18.0\n35.0\n0.0\n0.0\n0.0\n100.0\n105.0\n\n\n25%\n2.0\n36.0\n28.0\n35.0\n80.0\n0.0\n0.0\n700.0\n1117.0\n\n\n50%\n5.0\n48.0\n36.0\n51.0\n120.0\n3000.0\n0.0\n1000.0\n1400.0\n\n\n75%\n12.0\n60.0\n45.0\n72.0\n165.0\n6000.0\n0.0\n1300.0\n1692.0\n\n\nmax\n48.0\n72.0\n68.0\n180.0\n959.0\n300000.0\n30000.0\n5000.0\n11140.0\n\n\n\n\n\n\n\n\n\nData Splitting\nWe divide the date to three sets: train, validation and test.\n\nfrom sklearn.model_selection import train_test_split\n\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=11)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=11)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = (df_train.status == 'default').astype('int').values\ny_val = (df_val.status == 'default').astype('int').values\ny_test = (df_test.status == 'default').astype('int').values\n\ndel df_train['status']\ndel df_val['status']\ndel df_test['status']\n\nTurn pandas dataframe to dictionary format.\n\nfrom sklearn.feature_extraction import DictVectorizer\n\n# Filling missing values with 0\ndict_train = df_train.fillna(0).to_dict(orient='records')\ndict_val = df_val.fillna(0).to_dict(orient='records')\n\nSlice the first dictionary of feature values from training set to see what it looks like.\n\ndict_train[0]\n\n{'seniority': 10,\n 'home': 'owner',\n 'time': 36,\n 'age': 36,\n 'marital': 'married',\n 'records': 'no',\n 'job': 'freelance',\n 'expenses': 75,\n 'income': 0.0,\n 'assets': 10000.0,\n 'debt': 0.0,\n 'amount': 1000,\n 'price': 1400}\n\n\nDictVectorizer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays.\n\ndv = DictVectorizer(sparse=False)\n\nX_train = dv.fit_transform(dict_train)\nX_val = dv.transform(dict_val)\n\n\nX_train[0]\n\narray([3.6e+01, 1.0e+03, 1.0e+04, 0.0e+00, 7.5e+01, 0.0e+00, 0.0e+00,\n       1.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n       1.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 1.0e+00, 0.0e+00,\n       0.0e+00, 0.0e+00, 0.0e+00, 1.4e+03, 1.0e+00, 0.0e+00, 1.0e+01,\n       3.6e+01])\n\n\n\n\nGradient Boosting and XGBoost\n\nGradient boosting vs random forest\nInstalling XGBoost\nTraining the first model\nPerformance monitoring\nParsing xgboost’s monitoring output\n\nXGBoost requires a data structure suitable for both memory efficieny and training speed. Thankfully, XGBoost provides a way to convert different data sources to the structure DMatrix.\nNote:\nFeature names is no longer required to construct train and validation matrices\nIt was\nfeatures = dv.get_feature_names_out()\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=features)\nNow it’s\ndtrain = xgb.DMatrix(X_train, label=y_train)\n\nimport xgboost as xgb\n\nfeatures = dv.get_feature_names_out()\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\n\nBegin fitting XGBoost to the data.\n\n# Provide parameters for XGBoost\nxgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n    \n    'objective': 'binary:logistic',\n    'nthread': 8,\n    \n    'seed': 1,\n    'verbosity': 1,\n}\n\n# Train XGBoost to train matrices\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=50)\n\nPredict XGBoost model that has been trained on the validation set.\n\ny_pred = model.predict(dval)\ny_pred[:10]\n\narray([0.04114904, 0.01547354, 0.02859539, 0.14339729, 0.0053675 ,\n       0.03264806, 0.00689159, 0.53664714, 0.52047443, 0.00165206],\n      dtype=float32)\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_val, y_pred)\n\n0.8114570971882493\n\n\n\nwatchlist = [(dtrain, 'train'), (dval, 'val')]\n\n\n%%capture output\n\nxgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n    \n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n\n    'nthread': 8,\n    'seed': 1,\n    'verbosity': 1,\n}\n\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                  verbose_eval=10,\n                  evals=watchlist)\n\n\ns = output.stdout\n\nprint(s[:100])\n\n[0] train-auc:0.86300   val-auc:0.76818\n[10]    train-auc:0.95002   val-auc:0.81558\n[20]    train-auc:0.97316   \n\n\n\ndef parse_xgb_output(output):\n    results = []\n\n    for line in output.stdout.strip().split('\\n'):\n        it_line, train_line, val_line = line.split('\\t')\n\n        it = int(it_line.strip('[]'))\n        train = float(train_line.split(':')[1])\n        val = float(val_line.split(':')[1])\n\n        results.append((it, train, val))\n    \n    columns = ['num_iter', 'train_auc', 'val_auc']\n    df_results = pd.DataFrame(results, columns=columns)\n    return df_results\n\n\ndf_score = parse_xgb_output(output)\n\n\nplt.plot(df_score.num_iter, df_score.train_auc, color=\"green\", linestyle=\"solid\", label='Train AUC')\nplt.plot(df_score.num_iter, df_score.val_auc, color=\"orange\", linestyle=\"dashed\", label='Validation AUC')\nplt.legend()\n\nplt.title('XGBoost: number of trees vs AUC')\nplt.xlabel('Number of trees')\nplt.ylabel('AUC')\n\n# plt.savefig('ch06-figures/06_xgb_default.svg')\n\nplt.show()\n\n\n\n\n\nplt.plot(df_score.num_iter, df_score.val_auc, label='Validation AUC')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1dc4a0b0550&gt;\n\n\n\n\n\n\n\nSaving a XGBoost model with BentoML\nAfter training, use save_model() to save the Booster instance to BentoML model store. XGBoost has no framework-specific save options.\n\nimport bentoml\n\nbentoml.xgboost.save_model(\"credit_scoring_model\", model,\n                        custom_objects={\n                            \"dictVectorizer\": dv\n                            })\n\nModel(tag=\"credit_scoring_model:xbdou5qemcv5o7fs\", path=\"C:\\Users\\rizdi\\bentoml\\models\\credit_scoring_model\\xbdou5qemcv5o7fs\\\")\n\n\n\n### If prediction on batches is preferred\n\nbentoml.xgboost.save_model(\"credit_scoring_model\", model,\n                            custom_objects={\n                                \"dictVectorizer\": dv\n                            },\n                            signatures={\n                                \"predict\": {\n                                    \"batchable\": True,\n                                    \"batch_dim\": 0,\n                                }\n                            }\n                        )"
  }
]